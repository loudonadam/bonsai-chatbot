model:
  path: "models/bonsai-gguf.gguf"  # Path to your GGUF model file
  server_binary: "C:\\Users\\loudo\\Desktop\\src\\llama.cpp\\build\\bin\\Release\\llama-server.exe"  # Path to llama.cpp's llama-server.exe (optional helper)
  api_base: "http://127.0.0.1:8080/v1"  # llama.cpp server URL (OpenAI-compatible)
  name: "local-llm"
  max_tokens: 512
  temperature: 0.7
  timeout_seconds: 120
server:
  host: "0.0.0.0"
  port: 8010
ui:
  host: "127.0.0.1"
  port: 3000
embedding:
  model: "BAAI/bge-small-en-v1.5"
  device: "cpu"  # set to "cuda", "dml", or similar if you install GPU-capable torch/onnxruntime
  cache_dir: null  # set to a folder path to store/download embedding models
  local_files_only: false  # set true to require embeddings to be available locally (no internet)
retrieval:
  k: 4
  chunk_size: 800
  chunk_overlap: 120
data:
  raw_dir: "data/raw"
  index_dir: "data/index"
